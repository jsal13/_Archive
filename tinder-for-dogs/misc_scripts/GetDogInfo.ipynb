{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_LIST_BASE = \"https://www.dogbreedslist.info/all-dog-breeds/list_1_{}.html\"  # {} in [1, 19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dog_list_page(n):\n",
    "    r = requests.get(URL_LIST_BASE.format(n))\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    divs = soup.find_all('div', {'class': 'left'})\n",
    "    dog_urls = [div.find_all('a')[0][\"href\"] for div in divs]\n",
    "    return [(re.findall(\"/all-dog-breeds/(.*)?\\.html\", url)[0], url) for url in dog_urls]\n",
    "\n",
    "def load_dog_list_page():\n",
    "    return json.load(open(\"./dog_url_list.json\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL_LIST_ALL_DOGS = []\n",
    "# for i in range(1, 20):\n",
    "#     URL_LIST_ALL_DOGS.extend(get_dog_list_page(i))\n",
    "#     time.sleep(2)\n",
    "\n",
    "# json.dump(URL_LIST_ALL_DOGS, open(\"./dog_url_list.json\", \"w+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_LIST_ALL_DOGS_loaded = load_dog_list_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dog_info(n):\n",
    "    \"\"\"Gets breed-specific info.  Unfortunately, numbered by index.\"\"\"\n",
    "    dog = URL_LIST_ALL_DOGS_loaded[n]\n",
    "    r = requests.get(dog[1])\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "    info_table = soup.find_all('table', {'class': 'table-01'})\n",
    "    characteristics_table = soup.find_all('table', {'class': 'table-02'})\n",
    "    \n",
    "    dog_classes = [[i for i in tr.contents if i != '\\n'] for tr in characteristics_table[0].find_all(\"tr\")][1:]\n",
    "    characteristics = {dog_class[0].string: dog_class[1].p.contents[0][0] for dog_class in dog_classes}\n",
    "    info = dict([[i.string for i in info_table[0].find_all(\"tr\")[2:][i].contents if i != '\\n'] for i in [0, 4]])\n",
    "    dog_info = [info[\"Name\"], {**characteristics, **{'Size': info[\"Size\"]}}]\n",
    "\n",
    "    return dog_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332 list index out of range\n",
      "369 list index out of range\n"
     ]
    }
   ],
   "source": [
    "# ALL_DOG_INFO = {}\n",
    "# for i in range(len(URL_LIST_ALL_DOGS_loaded)):\n",
    "#     try:\n",
    "#         time.sleep(1)\n",
    "#         gdi = get_dog_info(i)\n",
    "#         ALL_DOG_INFO[gdi[0]] = gdi[1]\n",
    "#     except Exception as e:\n",
    "#         print(i, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SPECIAL CASES DUE TO BAD HTML.\n",
    "\n",
    "# dog = URL_LIST_ALL_DOGS_loaded[332]\n",
    "# r = requests.get(dog[1])\n",
    "# soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "# info_table = soup.find_all('table', {'class': 'table-01'})\n",
    "# characteristics_table = soup.find_all('table', {'class': 'table-02'})\n",
    "\n",
    "# dog_classes = [[i for i in tr.contents if i != '\\n'] for tr in characteristics_table[0].find_all(\"tr\")][1:]\n",
    "# characteristics = {dog_class[0].string: dog_class[1].p.contents[0][0] for dog_class in dog_classes}\n",
    "# # info = dict([[i.string for i in info_table[0].find_all(\"tr\")[2:][i].contents if i != '\\n'] for i in [0, 4]])\n",
    "# dog_info = {**characteristics, **{\"Size\": \"Medium\"}}\n",
    "# ALL_DOG_INFO[\"American Husky\"] = dog_info\n",
    "\n",
    "# dog = URL_LIST_ALL_DOGS_loaded[369]\n",
    "# r = requests.get(dog[1])\n",
    "# soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "# info_table = soup.find_all('table', {'class': 'table-01'})\n",
    "# characteristics_table = soup.find_all('table', {'class': 'table-02'})\n",
    "\n",
    "# dog_classes = [[i for i in tr.contents if i != '\\n'] for tr in characteristics_table[0].find_all(\"tr\")][1:]\n",
    "# characteristics = {dog_class[0].string: dog_class[1].p.contents[0][0] for dog_class in dog_classes}\n",
    "# # info = dict([[i.string for i in info_table[0].find_all(\"tr\")[2:][i].contents if i != '\\n'] for i in [0, 4]])\n",
    "# dog_info = {**characteristics, **{\"Size\": \"Medium\"}}\n",
    "# ALL_DOG_INFO[\"Mountain Cur\"] = dog_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(ALL_DOG_INFO, open(\"./dog_metadata.json\", \"w+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
